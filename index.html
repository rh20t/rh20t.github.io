<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="RH20T">
  <meta name="keywords" content="RH20T, Robotics, Robot Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RH20T: A Comprehensive Robotic Dataset for Learning Diverse Skills in One-Shot</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://rh20t.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://graspnet.net/anygrasp.html">
            AnyGrasp
          </a>
          <a class="navbar-item" href="https://airexo.github.io/">
            AirExo
          </a>
          <a class="navbar-item" href="https://rise-policy.github.io/">
            RISE
          </a>
          <a class="navbar-item" href="https://cage-policy.github.io/">
            CAGE
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">RH20T: A Comprehensive Robotic Dataset for Learning Diverse
            Skills in One-Shot</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://fang-haoshu.github.io/">Hao-Shu Fang</a>,</span>
            <span class="author-block">
              <a href="https://tonyfang.net/">Hongjie Fang</a>,</span>
            <span class="author-block">
              <a href="https://github.com/Vladimirovich2019">Zhenyu Tang</a>,
            </span>
            <span class="author-block">
              <a href="https://github.com/todibo99">Jirong Liu</a>,
            </span>
            <span class="author-block">
              <a href="https://github.com/chenxi-wang">Chenxi Wang</a>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://github.com/dadadadawjb">Junbo Wang</a>,
            </span>
            <span class="author-block">
              <a href="https://www.haoyizhu.site/">Haoyi Zhu</a>,
            </span>
            <span class="author-block">
              <a href="https://www.mvig.org/">Cewu Lu</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Shanghai Jiao Tong University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./static/RH20T_paper_compressed.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2307.00595"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!--
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/rh20t/rh20t_api"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>API</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="#download"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/plug.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        RH20T includes millions of <code>&lt;Human Demonstration, Robot Manipulation&gt;</code> pairs for each task.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            A key challenge in robotic manipulation in open domains is how to acquire diverse and generalizable skills for robots. Recent research in one-shot imitation learning has shown promise in transferring trained policies to new tasks based on demonstrations. This feature is attractive for enabling robots to acquire new skills and improving task and motion planning. However, due to limitations in the training dataset, the current focus of the community has mainly been on simple cases, such as push or pick-place tasks, relying solely on visual guidance. In reality, there are many complex skills, some of which may even require both visual and tactile perception to solve.
          </p>
          <p>
             This paper aims to unlock the potential for an agent to generalize to hundreds of real-world skills with multi-modal perception. To achieve this, we have collected a dataset comprising over 110,000 contact-rich robot manipulation sequences across diverse skills, contexts, robots, and camera viewpoints, all collected in the real world. Each sequence in the dataset includes visual, force, audio, and action information, along with a corresponding human demonstration video. We have invested significant efforts in calibrating all the sensors and ensuring a high-quality dataset. The dataset will be made publicly available.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Tasks</h2>
        <div class="content has-text-justified">
          <p>
            We select 48 tasks from <a href="https://sites.google.com/view/rlbench">RLBench</a>, 29 tasks from <a href="https://meta-world.github.io/">MetaWorld</a>, and introduce 70 self-proposed tasks that are frequently encountered and achievable by robots. Here are some selected tasks:
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-task208">
          <video poster="" id="task208" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/task208.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-task9">
          <video poster="" id="task" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/task9.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-task11">
          <video poster="" id="task11" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/task11.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-task19">
          <video poster="" id="task19" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/task19.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-task29">
          <video poster="" id="task29" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/task29.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-task30">
          <video poster="" id="task30" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/task30.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-task53">
          <video poster="" id="task53" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/task53.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-task63">
          <video poster="" id="task63" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/task63.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-task66">
          <video poster="" id="task66" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/task66.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-task86">
          <video poster="" id="task86" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/task86.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-task101">
          <video poster="" id="task101" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/task101.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-task112">
          <video poster="" id="task112" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/task112.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-task127">
          <video poster="" id="task127" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/task127.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Tele-Operation</h2>
        <div class="column">
          <div class="columns is-centered">
            <div class="column content has-text-justified">
              <p>
                Unlike previous methods that simplify the tele-operation interface using 3D mice, VR remotes, or mobile phones, we place emphasis on the importance of intuitive and accurate tele-operation in collecting contact-rich robot manipulation data. 
              </p>
            </div>
          </div>
        </div>

      </div>
    </div>
  </div>
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Setup. -->
      <div class="column has-text-centered">
        <div class="content">
          <h4 class="title">Platform Setup</h4>
          <image src="./static/images/teleop0.png" height="60px">
        </div>
        <div class="column content has-text-justified">
          <p>
            Each platform contains a robot arm with force-torque sensor, gripper and 1-2 inhand cameras, 8-10 global RGBD cameras and 2 microphones for data collection. A haptic device and a pedal are utilized to allow the operator to tele-operate the robot intuitively. These devices are all linked to a data collection workstation. 
          </p>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column has-text-centered">
        <div class="content">
          <h4 class="title">Data Collection</h4>
          <video id="dollyzoom" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/teleop2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Data Details</h2>
        <h4 class="title is-4">Overview</h4>
        <div class="content has-text-justified">
          <p>
            The following table depicts the data modality in our dataset. The last modality of fingertip tactile sensing is only available in robot Cfg. 7.
          </p>
          <style type="text/css">
            table.tftable {font-size:15px;color:#333333;width:100%;border-width: 1px;border-color: #729ea5;border-collapse: collapse;}
            table.tftable th {font-size:15px;background-color:#acc8cc;border-width: 1px;padding: 8px;border-style: solid;border-color: #729ea5;text-align:left;}
            table.tftable tr {background-color:#ffffff;}
            table.tftable td {font-size:15px;border-width: 1px;padding: 8px;border-style: solid;border-color: #729ea5;}
            </style>
            
            <table id="tfhover" class="tftable" border="1">
            <tr><th>Modal</th><th>Size</th><th>Frequency</th></tr>
            <tr><td>RGB image</td><td>1280 x 720 x 3</td><td>10 Hz</td></tr>
            <tr><td>Depth image</td><td>1280 x 720</td><td>10 Hz</td></tr>
            <tr><td>Binocular IR images</td><td>2 x 1280 x 720</td><td>10 Hz</td></tr>
            <tr><td>Robot joint angle</td><td>6 / 7</td><td>10 Hz</td></tr>
            <tr><td>Robot joint torque</td><td>6 / 7</td><td>10 Hz</td></tr>
            <tr><td>Gripper Cartesian pose</td><td>6 / 7 </td><td>100 Hz</td></tr>
            <tr><td>Gripper width</td><td>1</td><td>10 Hz</td></tr>
            <tr><td>6-DoF Force/Torque</td><td>6</td><td>100 Hz</td></tr>
            <tr><td>Audio</td><td>N/A</td><td>30 Hz</td></tr>
            <tr><td>Fingertip tactile</td><td>2 x 16 x 3</td><td>200 Hz</td></tr>
            </table>
            <p><small>The sizes of the robot joint angle, the robot joint torque and the gripper Cartesian pose depend on the robot type.</small></p>
            
        </div>
        <h4 class="title is-4">Sample</h4>
        <div class="content has-text-justified">
          <p>
            Here is a sample visualization of data, including RGBD images and binocular infrared images.
          </p>
          <video id="dollyzoom" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/vis-img.mp4"
                    type="video/mp4">
          </video>
        </div>
        <h4 class="title is-4">Point Cloud</h4>
        <div class="content has-text-justified">
          <p>
            We visualize the point cloud generated by fusing the RGBD data from these multi-view cameras. The red pyramids indicate the camera poses. Additionally, the robot model is rendered in the scene based on the joint angles recorded in our dataset. In the following videos, it is evident that all the cameras are calibrated with respect to the robot's base frame, and all the recorded data are synchronized in the temporal domain. The details of the robot configuration (Robot Cfg) can be found in the paper appendix.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-task6">
          <video poster="" id="task6-vis"  autoplay controls muted loop playsinline  height="100%">
            <source src="./static/videos/vis1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-task207">
          <video poster="" id="task207-vis"  autoplay controls muted loop playsinline  height="100%">
            <source src="./static/videos/vis2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-task41">
          <video poster="" id="task41-vis"  autoplay controls muted loop playsinline  height="100%">
            <source src="./static/videos/vis3.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-task96">
          <video poster="" id="task96-vis" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/vis4.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-task52">
          <video poster="" id="task52-vis" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/vis5.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-task12">
          <video poster="" id="task12-vis" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/vis6.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"><a id="download">Download</a></h2>
        <div class="content has-text-justified">
        <p><b>Caution: The RH20T dataset comprises volunteer-recorded human-robot interactions, possibly featuring volunteers' faces and voices. Exercise care to avoid inspecting or sharing sensitive content; kindly utilize the dataset solely for model training purposes.
        </b></p>
        <br>
        <p><b>Note: </b>We provide a 640x360-resized version of our dataset as the original size is too large (40TB). After extraction, the current dataset size is ~5TB for RGB and ~10TB for RGBD. <i>Depth images may have inaccuracies due to compression in this version</i>. You can use <a href="https://rclone.org/">Rclone</a> to download files from Gdrive according to <a href="https://github.com/rh20t/rh20t_api/issues/4">this thread</a>. We also provide a very well formatted data parsing and visualization <a href="https://github.com/rh20t/rh20t_api">API</a> to decode and use this dataset. 
        </p>
        <br>
          <p>
            <b><a href="./static/task_description.json">Task Description File</a></b> <br> <br>
            <b>RGB with Robot Infomation:</b> <br>
            RH20T_cfg1.tar.gz (178GB) (<a href="https://drive.google.com/file/d/1xbFMNQDYZKMf_jL4f6e06iT95BZQe4eG/view?usp=sharing">Google Drive</a>|<a href="https://pan.baidu.com/s/1a-Ex9653TWNr8N5e4jcXsw?pwd=gok9">Baidu Cloud</a>) <br>
            RH20T_cfg2.tar.gz (80GB) (<a href="https://drive.google.com/file/d/1dCRwmdn3cg2330zhY0lIPvG6Q9YGoCYz/view?usp=sharing">Google Drive</a>|<a href="https://pan.baidu.com/s/18Gfyyl4NTxVCauEb66N3Xg?pwd=9ugw">Baidu Cloud</a>) <br>
            patch.tar.gz (3GB) (<a href="https://drive.google.com/file/d/1nMYHHvwOUeWwJK-2zTlwz1fdZ7lqwio8/view?usp=sharing">Google Drive</a>|<a href="https://pan.baidu.com/s/1PFy959gyfrcS59JijXKycQ?pwd=u44f">Baidu Cloud</a>) (contains the camera calibration files and robot joint angles for cfg1 and cfg2, unzip and merge with cfg1 and cfg2 respectively.)<br>
            RH20T_cfg3.tar.gz (26GB) (<a href="https://drive.google.com/file/d/1uwieq-EbA_eTXE668ekypQV1cO9PDfES/view?usp=sharing">Google Drive</a>|<a href="https://pan.baidu.com/s/1PAJ8Ewqp8xNIPughHuHZJw?pwd=z8qq">Baidu Cloud</a>) <br>
            RH20T_cfg4.tar.gz (88GB) (<a href="https://drive.google.com/file/d/1fmVJMyiiKw8qOemU5FPzsW1NT3f5Kyjx/view?usp=sharing">Google Drive</a>|<a href="https://pan.baidu.com/s/1vjnqyZtVk3Zq0llM1f_7mA?pwd=vi7m">Baidu Cloud</a>) <br>
            RH20T_cfg5.tar.gz (37GB) (<a href="https://drive.google.com/file/d/17QgZ2HNdOAzF4krJ4eegH1rWnUTXfWDm/view?usp=sharing">Google Drive</a>|<a href="https://pan.baidu.com/s/19eD1uyvydGyWenNHb5ffVA?pwd=kqpj">Baidu Cloud</a>) <br>
            RH20T_cfg6.tar.gz (76GB) (<a href="https://drive.google.com/file/d/1Ytio7KTeU4gFlZNzl0-oX8wG-57VAbE9/view?usp=sharing">Google Drive</a>|<a href="https://pan.baidu.com/s/1JlIVz12bQSTTTiyXYCDIiA?pwd=h6io">Baidu Cloud</a>) <br>
            RH20T_cfg7.tar.gz (37GB) (<a href="https://drive.google.com/file/d/1ddwXNcRV3oi2mpMTLyhDvttwdd0lGRgX/view?usp=sharing">Google Drive</a>|<a href="https://pan.baidu.com/s/1i2WINgYZUzQWzcsMSGiyYA?pwd=009p">Baidu Cloud</a>)  <br>
            <br>
            <b>Depth:</b> <br> 
            RH20T_cfg1_depth.tar.gz (228GB) (<a href="https://drive.google.com/file/d/1RcglSD0_S10xsIwQVmJhC0flAQ9WZtIM/view?usp=sharing">Google Drive</a>|<a href="https://pan.baidu.com/s/1BfzadPmNG9V414ym_azrzQ?pwd=9avv">Baidu Cloud</a>) <br>
            RH20T_cfg2_depth.tar.gz (108GB) (<a href="https://drive.google.com/file/d/1Z-g-A7Smlxi4AI6h9nJ04bcdHmGiSWQK/view?usp=sharing">Google Drive</a>|<a href="https://pan.baidu.com/s/16TCQXwhypcr8VIPDUUaUMg?pwd=vaf3">Baidu Cloud</a>) <br>
            RH20T_cfg3_depth.tar.gz (26GB) (<a href="https://drive.google.com/file/d/1aekLEcX1ruS9f2z6900ys5t_U_OJnEzQ/view?usp=sharing">Google Drive</a>|<a href="https://pan.baidu.com/s/1c4-aH3v4YTAD7IAl2ZkqNA?pwd=ecta">Baidu Cloud</a>) <br>
            RH20T_cfg4_depth.tar.gz (83GB) (<a href="https://drive.google.com/file/d/1cPuQ9LV_Kn2BEcr_a5AKd8or5xRvioYA/view?usp=sharing">Google Drive</a>|<a href="https://pan.baidu.com/s/1MtBDvTwZ9n6WKIGgem7g_Q?pwd=cotz">Baidu Cloud</a>) <br>
            RH20T_cfg5_depth.tar.gz (66GB) (<a href="https://drive.google.com/file/d/1-TCrstA_FMuYqb9SbmFnos6f8kuzlkQM/view?usp=sharing">Google Drive</a>|<a href="https://pan.baidu.com/s/1jCsY0ndP9WxWrH7U1LlTdA?pwd=b5er">Baidu Cloud</a>) <br>
            RH20T_cfg6_depth.tar.gz (99GB) (<a href="https://drive.google.com/file/d/1dFVSjah4LeH2Liy2hVuYuxjd-4vZ_n4H/view?usp=sharing">Google Drive</a>|<a href="https://pan.baidu.com/s/1mCIVrwKicHWj3uPSrCgugA?pwd=pw7u">Baidu Cloud</a>) <br>
            RH20T_cfg7_depth.tar.gz (41GB) (<a href="https://drive.google.com/file/d/1o4nmbzkfw9EiWwP0Ri6E4Q75cgipZZhh/view?usp=sharing">Google Drive</a>|<a href="https://pan.baidu.com/s/15rOObVdZo_pZQZZrWjJgnw?pwd=16v6">Baidu Cloud</a>) <br>
          </p>
          <br>
          <hr>
          <br>
          <p><b>Note 2: </b>A 320x180-resized version of our dataset is also available. In this version, RGB images are video-compressed, while depth images use lossless compression for more accurate 3D data. This version is ideal if you want to utilize the precise real-world 3D information.
          </p>
          <br>
          <p>
            <b>RGB:</b> <br>
            RH20T_cfg1.tar.gz (30.3GB) (Google Drive | <a href="https://pan.baidu.com/s/1L-NRwvqFGt4TX0DplNvmOQ?pwd=rfwp">Baidu Cloud</a>) <br>
            RH20T_cfg2.tar.gz (14.8GB) (Google Drive | <a href="https://pan.baidu.com/s/1sFT7JlQT3_6ehJZK5fnUYQ?pwd=di6n">Baidu Cloud</a>) <br>
            RH20T_cfg3.tar.gz (4.4GB) (Google Drive | <a href="https://pan.baidu.com/s/1BXFNfJRP-Xel5R7ihuTP2g?pwd=dssr">Baidu Cloud</a>) <br>
            RH20T_cfg4.tar.gz (14.7GB) (Google Drive | <a href="https://pan.baidu.com/s/1mo3KXRIqXDcz11HUFvnJQg?pwd=erja">Baidu Cloud</a>) <br>
            RH20T_cfg5.tar.gz (8.2GB) (Google Drive | <a href="https://pan.baidu.com/s/11A-B0fGH1v8wfR9mGXadvw?pwd=jib9">Baidu Cloud</a>) <br>
            RH20T_cfg6.tar.gz (13.6GB) (Google Drive | <a href="https://pan.baidu.com/s/1K9iS3EqZzPsGgRoOj1ZIVQ?pwd=53uc">Baidu Cloud</a>) <br>
            RH20T_cfg7.tar.gz (6.7GB) (Google Drive | <a href="https://pan.baidu.com/s/1QHcjf7cO_Wjvim9uPtYrKQ?pwd=jddz">Baidu Cloud</a>)  <br>
            <br>
            <b>Depth:</b> <br>
            RH20T_cfg1.tar.gz (572.2GB) (Google Drive | <a href="https://pan.baidu.com/s/1a84PN4OKd2ImX9Mls_qS2g?pwd=jwrj">Baidu Cloud</a>) <br>
            RH20T_cfg2.tar.gz (319.8GB) (Google Drive | <a href="https://pan.baidu.com/s/1422a71Bq5Rrj2i0_0Jpeow?pwd=3r52">Baidu Cloud</a>) <br>
            RH20T_cfg3.tar.gz (71.3GB) (Google Drive | <a href="https://pan.baidu.com/s/1OoDQDHQLOuO-cPAM3NSTKg?pwd=9v8j">Baidu Cloud</a>) <br>
            RH20T_cfg4.tar.gz (227.9GB) (Google Drive | <a href="https://pan.baidu.com/s/1i00ym9AxjQdltP-RCxGN6A?pwd=sqgj">Baidu Cloud</a>) <br>
            RH20T_cfg5.tar.gz (200.7GB) (Google Drive | <a href="https://pan.baidu.com/s/18x7DWal9YqxiQ4mf4KBm4A?pwd=zm8s">Baidu Cloud</a>) <br>
            RH20T_cfg6.tar.gz (272.4GB) (Google Drive | <a href="https://pan.baidu.com/s/18liyYEKG3zEf8z8EY-gKfw?pwd=59wr">Baidu Cloud</a>) <br>
            RH20T_cfg7.tar.gz (96.0GB) (Google Drive | <a href="https://pan.baidu.com/s/1rC0Ge7Wa2ASjQpKyLLvgvQ?pwd=u2v6">Baidu Cloud</a>)  <br>
            <br>
            <b>LowDim:</b> <br>
            RH20T_cfg1.tar.gz (79.9GB) (Google Drive | <a href="https://pan.baidu.com/s/17ot4Hw8r-csuLUar7UA1Sw?pwd=wqp2">Baidu Cloud</a>) <br>
            RH20T_cfg2.tar.gz (31.9GB) (Google Drive | <a href="https://pan.baidu.com/s/1PNw4SRk2ksIFcx8YeNobdQ?pwd=jvg4">Baidu Cloud</a>) <br>
            RH20T_cfg3.tar.gz (11.3GB) (Google Drive | <a href="https://pan.baidu.com/s/1YK3xcHKRhufrLg7rLZ36Og?pwd=nj6v">Baidu Cloud</a>) <br>
            RH20T_cfg4.tar.gz (38.6GB) (Google Drive | <a href="https://pan.baidu.com/s/1QiMA82k-bAYZb9Bhz4Fbqw?pwd=mknr">Baidu Cloud</a>) <br>
            RH20T_cfg5.tar.gz (6.3GB) (Google Drive | <a href="https://pan.baidu.com/s/1nfF1iYXSpJCC_dwlEAvijA?pwd=dnca">Baidu Cloud</a>) <br>
            RH20T_cfg6.tar.gz (31.3GB) (Google Drive | <a href="https://pan.baidu.com/s/1Q28Ny283G3H7P1iXFkD5Sg?pwd=db7d">Baidu Cloud</a>) <br>
            RH20T_cfg7.tar.gz (14.9GB) (Google Drive | <a href="https://pan.baidu.com/s/1ngDCGx6OyZz2ljzGxAH_vg?pwd=ewts">Baidu Cloud</a>)  <br>
            <br>
            <b>Calibration:</b> <br>
            RH20T_cfg1.tar.gz (805.6MB) (Google Drive | <a href="https://pan.baidu.com/s/14ME9yPLU_aZ5Q9KZjUCpTw?pwd=4ejg">Baidu Cloud</a>) <br>
            RH20T_cfg2.tar.gz (584.0MB) (Google Drive | <a href="https://pan.baidu.com/s/15Jq28AQ0qZ6FZqprnNtNsA?pwd=r4te">Baidu Cloud</a>) <br>
            RH20T_cfg3.tar.gz (334.7MB) (Google Drive | <a href="https://pan.baidu.com/s/1iEYBxvPMh86VpZba-g4Rnw?pwd=529s">Baidu Cloud</a>) <br>
            RH20T_cfg4.tar.gz (391.6MB) (Google Drive | <a href="https://pan.baidu.com/s/1ti685qpqN-M6hROtma_niQ?pwd=iacq">Baidu Cloud</a>) <br>
            RH20T_cfg5.tar.gz (79.1MB) (Google Drive | <a href="https://pan.baidu.com/s/1k_FQtUM5Qk4q9JeBPvupCQ?pwd=zuvg">Baidu Cloud</a>) <br>
            RH20T_cfg6.tar.gz (79.6MB) (Google Drive | <a href="https://pan.baidu.com/s/1aaLzc1bEpRi4Outk2ySVyQ?pwd=tgvq">Baidu Cloud</a>) <br>
            RH20T_cfg7.tar.gz (14.9MB) (Google Drive | <a href="https://pan.baidu.com/s/1ZdSsWNKro81pyuFJfrhQBA?pwd=mcqr">Baidu Cloud</a>)  <br>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="Format">
  <div class="container is-max-desktop content">
    <div class="columns is-centered has-text-centered">
    <h2 class="title is-3">Data Format</h2>
  </div>
    <pre><code>
|-- RH20T
    |-- RH20T_cfg1
    |   |-- calib/                            # Calibration folder, including calibration-time Gripper Cartesian pose, intrinsic and extrinsic matrices etc. Extrinsic matrices are the Aruco marker's Translations with respect to the camera frame.
    |   |-- task_0001_user_0001_scene_0001_cfg_0001/        # Robotic manipulation data
    |   |    |-- metadata.json                # Robot manipulation scene metadata, including scene finishing timestamp, task completion rating (0 denotes robot failure, 1 denotes task failure, 2-9 denotes completion quality, higher is better), calibration timestamp and calibration quality (0 means some cameras are not calibrated, 1-5 means calibration accuracy, lower is better), etc.
    |   |    |-- cam_[serial_number]/         # Multiple cameras
    |   |    |    |-- color.mp4               # Color images, encode as video. The extraction code is available in our API code.
    |   |    |    |-- timestamps.npy          # Timestamp for each image, our extraction code will use it to decode images.
    |   |    |    `-- depth.mp4 (optional)    # Depth images, encode as video. The extraction code is available in our API code.
    |   |    |-- transformed/
    |   |    |    |-- tcp.npy                 # Gripper Cartesian pose in each cam's coord, {serial number: [{"timestamp": ..., "tcp": ..., "robot_ft": ...}]}, where "tcp" values are xyz+quat (7D) Gripper Cartesian poses
    |   |    |    |-- tcp_base.npy            # Gripper Cartesian pose in base coord, {serial number: [{"timestamp": ..., "tcp": ..., "robot_ft": ...}]}, where "tcp" values are xyz+quat (7D) Gripper Cartesian poses
    |   |    |    |-- joint.npy               # Joint angles, {serial number: {timestamp: joint angle array}}
    |   |    |    |-- gripper.npy             # Gripper commands and information, {serial number: {timestamp: {"gripper_command": 3D array, "gripper_info": 3D array}}}, where the 1st element in the 3D array is the actual gripper width in millimeters(0-110)
    |   |    |    |-- force_torque.npy        # 6-DoF force/torque in cam's coord, {serial number: [{"timestamp": ..., "zeroed": ..., "raw": ...}]}, where "zeroed" values are pre-processed
    |   |    |    |-- force_torque_base.npy   # 6-DoF force/torque in base coord, {serial number: [{"timestamp": ..., "zeroed": ..., "raw": ...}]}, where "zeroed" values are pre-processed
    |   |    |    `-- high_freq_data.npy      # High frequency data, {serial number: [{"timestamp": ..., "zeroed": ..., "raw": ..., "tcp": ...}]}
    |   |    `-- audio_mixed/
    |   |
    |   |-- task_0001_user_0001_scene_0001_cfg_0001_human/  # Human demonstration data corresponds to the above robotic manipulation
    |   |    |-- metadata.json                # Human demonstration metadata, including scene starting and finishing timestamps, calibration timestamp and quality
    |   |    |-- cam_[serial_number]/
    |   |    |    |-- color.mp4
    |   |    |    |-- timestamps.npy
    |   |    |    `-- depth.mp4 (optional)
    |   |    `-- audio_mixed/
    |   |
    |   |
    |   `-- ... ...
    |
    |
    |-- RH20T_cfg2/
    |   `-- same as above
    |
    |
    |-- ...
    |
    |
    `-- RH20T_cfg7/
</code></pre>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <div class="columns is-centered has-text-centered">
    <h2 class="title">BibTeX</h2>
    </div>
    <pre><code>@inproceedings{
  fang2024rh20t,
  title        = {RH20T: A Comprehensive Robotic Dataset for Learning Diverse Skills in One-Shot},
  author       = {Fang, Hao-Shu and Fang, Hongjie and Tang, Zhenyu and Liu, Jirong and Wang, Chenxi and Wang, Junbo and Zhu, Haoyi and Lu, Cewu},
  booktitle    = {2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages        = {653--660},
  year         = {2024},
  organization = {IEEE}
}
</code></pre>
  </div>
</section>

<section class="section" id="License">
  <div class="container is-max-desktop content">
    <div class="columns is-centered has-text-centered">
    <h2 class="title">License</h2>
    </div>
    <p>
    The dataset is licensed under a mixture of licenses as it is partly funded by a company. It is divided into two subsets: RH20T-C (commercial) and RH20T-NC (non-commercial).
    </p>
    <p>
    The RH20T-C subset contains episodes with names containing 'scene_0001' to 'scene_0005'. It is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">(CC BY-SA 4.0)</a>.
    </p>
    <p>
    The RH20T-NC subset contains episodes with names containing 'scene_0006' to 'scene_0010'. It is licensed under a Creative Commons Attribution 4.0 Non-Commercial License <a rel="license"
                                                href="http://creativecommons.org/licenses/by-nc-sa/4.0/">(CC BY-NC 4.0)</a>, which is freely available for free non-commercial use, and may be redistributed under these conditions. Commercial use of the RH20T-NC subset or models trained on it is not allowed.
    </p>
    <p>
    If you have any further questions, please contact <a href="mailto:fhaoshu@gmail.com">fhaoshu@gmail.com</a>.
    </p>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://openreview.net/pdf?id=YhRKICWgE9">
        <i class="fas fa-file-pdf"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>. Modified upon original <a href="https://nerfies.github.io/"> Nerfies </a> website (<a href="https://github.com/nerfies/nerfies.github.io">source</a>). The robot icons are created by <a href="https://www.flaticon.com/free-icons/robot" title="robot icons">smalllikeart @ Flaticon</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
